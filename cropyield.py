# -*- coding: utf-8 -*-
"""cropyield.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12O1G5hQi6MjTrM12jjLcX3BbuxsdZh74
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, roc_auc_score, mean_squared_error, accuracy_score
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
import matplotlib.pyplot as plt
import seaborn as sns

# Upload dataset
# Upload dataset
# Upload dataset
def upload_and_load_data():
    from google.colab import files  # Specific to Google Colab
    print("Please upload your dataset file:")
    uploaded = files.upload()
    file_name = list(uploaded.keys())[0]
    print(f"\nFile '{file_name}' uploaded successfully!")
    data = pd.read_csv(file_name)
    return data

# Preprocess dataset
def preprocess_data(data):
    print("Available columns in dataset:", data.columns.tolist())  # Debugging line
    original_data = data.copy()

    # Check for the target column dynamically
    possible_targets = ['Target', 'Yield', 'HighYield', 'Yield (tons/hectare)']  # Add your target column name here
    target_column = next((col for col in possible_targets if col in data.columns), None)
    if not target_column:
        raise KeyError("Target column not found. Ensure the dataset contains a target column.")

    # Discretize the target variable if continuous
    if data[target_column].dtype in ['float64', 'int64']:
        print(f"Discretizing continuous target column '{target_column}'...")
        threshold = data[target_column].median()  # Example threshold: median value
        data[target_column] = (data[target_column] > threshold).astype(int)

    # Identify numerical and categorical features dynamically
    numerical_features = data.select_dtypes(include=['int64', 'float64']).columns.tolist()
    categorical_features = data.select_dtypes(include=['object', 'category']).columns.tolist()

    # Exclude target from feature lists
    numerical_features = [col for col in numerical_features if col != target_column]
    categorical_features = [col for col in categorical_features if col != target_column]

    # Scale numerical features
    scaler = MinMaxScaler()
    data[numerical_features] = scaler.fit_transform(data[numerical_features])

    # One-hot encode categorical features
    data_encoded = pd.get_dummies(data, columns=categorical_features, drop_first=True)

    return data_encoded, original_data, target_column

# Define and apply multiple models
def train_evaluate_models(X_train, X_test, y_train, y_test):
    results = []

    # 1. Decision Tree
    dt = DecisionTreeClassifier()
    dt.fit(X_train, y_train)
    y_pred = dt.predict(X_test)
    results.append(evaluate_model("Decision Tree", dt, X_test, y_test, y_pred))

    # 2. Random Forest
    rf = RandomForestClassifier()
    rf.fit(X_train, y_train)
    y_pred = rf.predict(X_test)
    results.append(evaluate_model("Random Forest", rf, X_test, y_test, y_pred))

    # 3. Logistic Regression
    lr = LogisticRegression()
    lr.fit(X_train, y_train)
    y_pred = lr.predict(X_test)
    results.append(evaluate_model("Logistic Regression", lr, X_test, y_test, y_pred))

    # 4. ANN
    ann = Sequential([
        Dense(16, activation='relu', input_dim=X_train.shape[1]),
        Dense(8, activation='relu'),
        Dense(1, activation='sigmoid')
    ])
    ann.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    ann.fit(X_train, y_train, epochs=10, verbose=0)
    y_pred = (ann.predict(X_test) > 0.5).astype(int).flatten()
    results.append(evaluate_model("ANN", ann, X_test, y_test, y_pred))

    # 5. BRBES
    belief_rules = define_belief_rules()
    belief_degree = calculate_belief_degree(X_test, belief_rules)
    y_pred = (belief_degree > 0.5).astype(int)
    results.append(evaluate_model("BRBES", None, X_test, y_test, y_pred))

    return pd.DataFrame(results)

# Evaluate a model
def evaluate_model(name, model, X_test, y_test, y_pred):
    cm = confusion_matrix(y_test, y_pred)
    sensitivity = cm[1, 1] / (cm[1, 1] + cm[1, 0]) if cm[1, 1] + cm[1, 0] > 0 else 0
    specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1]) if cm[0, 0] + cm[0, 1] > 0 else 0
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    auc = roc_auc_score(y_test, y_pred)
    accuracy = accuracy_score(y_test, y_pred)

    return {
        "Model": name,
        "Sensitivity %": sensitivity * 100,
        "Specificity %": specificity * 100,
        "RMSE": rmse,
        "AUC": auc,
        "Accuracy %": accuracy * 100
    }

# Define belief rules
def define_belief_rules():
    """
    Example belief rules:
    - Temperature, rainfall, and humidity should be within specific ranges.
    - Soil type and weather conditions influence the belief degree.
    """
    return [
        {"rule": "Optimal temperature and rainfall", "weight": 0.7, "conditions": lambda x: (20 <= x['Temperature (Â°C)'] <= 30) and (100 <= x['Rainfall (mm)'] <= 200)},
        {"rule": "High humidity with suitable soil", "weight": 0.6, "conditions": lambda x: (60 <= x['Humidity (%)'] <= 80) and (x['Soil Type'] in ['Loamy', 'Clay'])},
        {"rule": "Favorable weather condition", "weight": 0.8, "conditions": lambda x: x.get('Weather Condition', None) == 'Sunny'}  # Use .get() to avoid KeyError
    ]

# Calculate belief degree
def calculate_belief_degree(data, rules):
    """
    Calculate belief degree based on the defined rules.
    :param data: DataFrame containing seed and environmental features.
    :param rules: List of belief rules.
    :return: Belief degree as a numpy array.
    """
    belief_degree = np.zeros(len(data))

    for rule in rules:
        weight = rule['weight']
        condition = rule['conditions']
        belief_degree += weight * data.apply(condition, axis=1)

    return np.clip(belief_degree, 0, 1)  # Ensure belief degree is between 0 and 1

# Main function
def main():
    # Step 1: Upload dataset
    data = upload_and_load_data()

    # Step 2: Preprocess dataset
    data_encoded, original_data, target_column = preprocess_data(data)
    X = data_encoded.drop(target_column, axis=1)
    y = data_encoded[target_column]

    # Step 3: Split dataset
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Step 4: Train and evaluate models
    model_results = train_evaluate_models(X_train, X_test, y_train, y_test)

    # Step 5: Display results
    print("\nModel Results:")
    print(model_results)

    # Step 6: Export results for table formatting
    model_results.to_csv("model_results.csv", index=False)
    print("\nResults saved as 'model_results.csv'")

if __name__ == "__main__":
    main()

"""# New Section"""
